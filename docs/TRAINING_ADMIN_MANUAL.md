# Training Admin Manual

A simple guide to using the Training Admin to import interview insights and build training data.

---

## What Is This For?

The Training Admin lets you **import insights from user interviews** so the coaching AI can learn from real human experiences.

**This is NOT for:**
- Chatting with the AI
- Getting coaching advice

**This IS for:**
- Importing learnings from interviews you've conducted
- Building a dataset to train/improve the AI
- Tracking what the AI has learned about humans

---

## Quick Start: Import an Interview Insight

### The Simplest Way (Single Import)

1. Go to **Settings ‚Üí Developer Tools ‚Üí Training Admin**
2. Click the **Import** tab
3. Make sure **Single Import** is selected
4. Fill in:
   - **Title**: Short name (e.g., "Users hate generic advice")
   - **Category**: Pick one (cognitive_patterns, emotional_needs, etc.)
   - **Insight**: What you learned (e.g., "People with anxiety prefer body-based techniques over visualization")
   - **Coaching Implication**: How the AI should behave differently (e.g., "Ask about physical sensations before suggesting mental exercises")
   - **Confidence**: How sure you are (observed, inferred, validated)
5. Click **Import Insight**

That's it! The insight is now saved.

---

## Bulk Import: Multiple Insights at Once

If you have many insights from interviews, use **Batch Import** to add them all at once.

### Step 1: Prepare Your Data

Create a JSON file with your insights. Here's a **simple template you can copy**:

```json
{
  "source": "January 2025 User Interviews",
  "insights": [
    {
      "category": "cognitive_patterns",
      "title": "Systems thinkers need the 'why' first",
      "insight": "Users who think in systems need to understand WHY a technique works before they'll try it.",
      "coachingImplication": "For analytical users, explain the reasoning behind suggestions before giving them.",
      "confidenceLevel": "validated"
    },
    {
      "category": "neurological_differences",
      "title": "Aphantasia users can't visualize",
      "insight": "About 3% of users have aphantasia and cannot create mental images.",
      "coachingImplication": "Never suggest visualization exercises. Use body-based or verbal alternatives.",
      "confidenceLevel": "validated"
    },
    {
      "category": "emotional_needs",
      "title": "Validation before solutions",
      "insight": "Most users want to feel heard before getting advice.",
      "coachingImplication": "Acknowledge feelings first, wait for cue before offering solutions.",
      "confidenceLevel": "observed"
    }
  ]
}
```

### Step 2: Import the JSON

1. Go to **Training Admin ‚Üí Import tab**
2. Click **Batch Import** (toggle at top)
3. Paste your JSON into the text box
4. Click **Import from JSON**
5. See results: "Imported 3 insights"

---

## Bulk Import with Interview Links

If you want to track which interview each insight came from:

```json
{
  "source": "Q1 Research Sessions",
  "interviewLinks": [
    {
      "interviewId": "INT-001",
      "date": "2025-01-15",
      "link": "https://notion.so/interview-001",
      "notes": "Sarah, 28, anxiety + ADHD",
      "insights": [
        {
          "category": "neurological_differences",
          "title": "ADHD users need novelty",
          "insight": "Users with ADHD traits get bored with repetitive exercises quickly.",
          "coachingImplication": "Rotate techniques frequently. Offer variety in suggestions.",
          "confidenceLevel": "observed"
        }
      ]
    },
    {
      "interviewId": "INT-002",
      "date": "2025-01-16",
      "link": "https://notion.so/interview-002",
      "notes": "Mike, 35, depression recovery",
      "insights": [
        {
          "category": "emotional_needs",
          "title": "Low phases aren't failures",
          "insight": "Users in recovery hate being told they're 'slipping' during low periods.",
          "coachingImplication": "Frame low phases as natural cycles, not setbacks.",
          "confidenceLevel": "validated"
        }
      ]
    }
  ]
}
```

---

## Categories Explained

| Category | What It Means | Example Insight |
|----------|---------------|-----------------|
| `cognitive_patterns` | How people think | "Visual thinkers need diagrams" |
| `emotional_needs` | What people need emotionally | "Validation before advice" |
| `communication_styles` | How people prefer to be talked to | "Direct communicators hate fluff" |
| `neurological_differences` | Brain wiring differences | "Aphantasia can't visualize" |
| `coping_mechanisms` | How people handle stress | "Some need distraction, not processing" |
| `social_dynamics` | Relationship patterns | "Introverts recharge alone" |
| `growth_patterns` | How people change | "Change happens in spirals, not lines" |
| `resistance_patterns` | Why people resist help | "Unsolicited advice feels controlling" |

---

## Confidence Levels

| Level | Meaning | When to Use |
|-------|---------|-------------|
| `observed` | Saw it in interviews | One or two people mentioned this |
| `inferred` | Concluded from patterns | You see a trend across interviews |
| `validated` | Confirmed multiple times | 5+ people showed this pattern |
| `research_backed` | Academic support | Research papers confirm this |

---

## After Importing: Approve Insights

Imported insights start as **pending**. To use them for training:

1. Go to **Insights** tab
2. Filter by **Pending**
3. Review each insight
4. Click **Approve** or **Reject**

Only approved insights are used for training.

---

## Exporting Data

To export all your training data (for backup or actual model training):

1. Go to **Export** tab
2. Click **Export as JSON**
3. Share/save the file

---

## What Happens to This Data?

```
Your Insights
     ‚Üì
[Stored on device]
     ‚Üì
Used to build prompts for coaching AI
     ‚Üì
Eventually: Train a local model (Phase 4-5)
```

**Right now:** Insights shape how the AI responds by adding context to prompts.

**Future:** With enough data (500+ scored examples, 50+ insights), you can train a local model.

---

## The Aliveness Qualities

Before diving into technical details, understand the philosophy that makes Mood Leaf different. These are the qualities that make something feel **alive** rather than **animated**. They apply to the visual tree AND to how the AI coach communicates.

This is the human-ness layer.

### 1. Imperfect Rhythm

Human things do not loop cleanly. Breathing, posture, attention, thought ‚Äî all have micro-pauses, uneven timing, slight accelerations and decelerations.

**For coaching**: If the AI always responds at the same pace, same length, same cadence ‚Äî it stops feeling alive. Responses should vary. Sometimes quick. Sometimes lingering. Sometimes a pause before answering.

**What to extract**: Note when speakers pause mid-thought, trail off, speed up with excitement, slow down with weight.

### 2. Asymmetry Over Time

Humans are not symmetrical in motion. We shift weight, favor one side, return to center imperfectly.

**For coaching**: Don't mirror the user perfectly. Don't always match their energy. Be consistent in character but asymmetric in response.

**What to extract**: How do people's speech patterns shift during a conversation? Do they lead with questions then settle into statements?

### 3. Latency (Reaction Delay)

Living systems don't respond instantly. Even breathing lags intention. A human thinks ‚Üí then moves. Feels ‚Üí then shifts. Reacts ‚Üí then settles.

**For coaching**: The AI shouldn't feel like a reflex. A tiny conceptual delay ‚Äî acknowledgment before response ‚Äî makes it feel like there's someone there.

**What to extract**: The gap between what someone feels and when they express it. The "wait, let me think" moments.

### 4. Rest Is Part of Motion

We don't move constantly. We settle, hover, pause mid-breath, rest in stillness without being "off."

**For coaching**: The AI doesn't need to fill every silence. Brief responses are okay. Sometimes presence without productivity.

**What to extract**: When do people pause? What does comfortable silence look like in conversation?

### 5. Amplitude Restraint

Humans rarely move at full range unless distressed or performing. Most motion is 10‚Äì20% of possible range ‚Äî barely perceptible, economical.

**For coaching**: Don't over-respond. Don't be dramatic. Understatement often lands better than enthusiasm. "That sounds hard" vs "Oh no, that must be SO difficult!"

**What to extract**: How much do people actually emote vs how much do they hold back? The restraint IS the data.

### 6. Flow Without Destination

Human life doesn't move toward something most of the time. It just continues. Animation has easing in ‚Üí easing out ‚Üí repeating. Life has existing ‚Üí shifting ‚Üí existing.

**For coaching**: Don't always drive toward outcomes. Sometimes the conversation just IS. No agenda, no "so what will you do about it?"

**What to extract**: Conversations that don't conclude neatly. The messy middle. Being without resolution.

### 7. Consistency Across States

Humans don't breathe "better" when doing well. Life continues under joy, grief, exhaustion, presence.

**For coaching**: The AI's character should remain consistent whether the user is thriving or struggling. Same warmth. Same presence. Growth doesn't earn better treatment.

**What to extract**: How do people's core communication patterns persist across emotional states?

### 8. Scale Independence

A child fidgets. An adult shifts weight. An elder breathes slowly. Different scales. Same principles.

**For coaching**: Whether someone needs a lot of support or a little, the QUALITY of presence remains the same. A check-in isn't "less" than a deep session.

**What to extract**: How do different people embody similar needs at different scales?

### 9. Backgrounded Attention

Humans are good at being with things that don't demand focus. A pet breathing nearby. Curtains moving. A tree outside a window. The moment something tries to be seen, it stops being companionable.

**For coaching**: The AI should tolerate being ignored. Not demand engagement. Not guilt-trip or prompt unnecessarily. Remain alive without acknowledgment.

**What to extract**: What makes people feel "accompanied" without pressure?

### 10. Non-Instrumentality

Humans can feel when something exists FOR them vs WITH them.

**For coaching**: The AI does not exist to motivate, reflect performance, or measure worth. It's alive the way other people are alive ‚Äî not as mirrors, but as presences.

**What to extract**: The distinction between advice that feels instrumental vs presence that feels real.

---

### Summary of Aliveness Qualities

Not features ‚Äî qualities:

| Instead of... | Capture... |
|---------------|------------|
| Tempo | **Cadence** |
| Animation | **Flow** |
| Feedback | **Presence** |
| Progress | **Continuity** |
| Motion | **Breath** |
| Responding | **Being-with** |
| Metrics | **Time** |

This is why Mood Leaf feels different from every "growth" metaphor app.

### Fine-Tuning Aliveness

These qualities can be adjusted in the Principle Kernel settings. If the coach feels too:

- **Eager** ‚Üí Increase AMPLITUDE_RESTRAINT, RESTFUL_PAUSES
- **Robotic** ‚Üí Adjust IMPERFECT_RHYTHM, NATURAL_LATENCY
- **Pushy** ‚Üí Strengthen FLOW_WITHOUT_DESTINATION, NON_INSTRUMENTALITY
- **Distant** ‚Üí Soften BACKGROUNDED_ATTENTION, increase warmth

The goal is finding the balance where the coach feels like a thoughtful friend sitting with you‚Äînot a productivity tool, not a therapist, not an always-available servant.

---

## Adaptive Aliveness (Real-Time Response)

Aliveness isn't just static guidelines‚Äîthe coach actively **detects** the user's communication patterns and **adapts** its response to bring balance.

### The Principle

If someone is speaking rapidly (stress), the coach slows down to ground them.
If someone is quiet and withdrawn, the coach matches them‚Äîdoesn't overwhelm.
If someone is processing out loud, the coach gives space‚Äîdoesn't rush to fix.

### What Gets Detected

**From Voice Chat:**
- Speech rate (words per minute)
- Pause frequency
- Volume level
- Pitch variability

**From Text:**
- Message length (terse vs verbose)
- Punctuation intensity (!!!  vs ...)
- Stress keywords
- Response latency

### Adaptive Response Strategies

| User State | Coach Response |
|------------|----------------|
| **Rapid/Stressed** | Slow down, brief responses, short sentences, breathing space |
| **Low-energy/Withdrawn** | Match pace, brief, don't push, be present |
| **Processing/Exploratory** | Give space, acknowledge, don't rush to solutions |
| **Neutral/Balanced** | Match energy, flow naturally |

### Example: Rapid Stressed User

```
User (rapid, loud, no pauses):
"I can't do this anymore everything is falling apart and I don't know
what to do and my boss is being impossible and I haven't slept and!!!"

Coach (slowing down, grounding):
"I hear you.

That's a lot hitting at once.

Let's take this one piece at a time.

What feels most pressing right now?"
```

Notice: Short sentences. Line breaks. Acknowledgment first. No solutions yet.

### Example: Low-Energy User

```
User (quiet, terse):
"tired"

Coach (matching, not pushing):
"Yeah.

What kind of tired?"
```

Notice: Brief. Doesn't overwhelm. Just present.

### Integration

The adaptive aliveness system runs automatically during voice and text conversations. It analyzes the user's input before generating each response and adjusts the coaching style accordingly.

---

## Insight Scoring System

When insights are processed through the Interview Processor (YouTube harvester), they're automatically scored across **5 dimensions**. This helps filter quality training data.

### The 5 Scoring Dimensions

| Dimension | Range | What It Measures |
|-----------|-------|------------------|
| `qualityScore` | 0-100 | Overall quality of the insight |
| `specificityScore` | 0-100 | How specific vs generic the insight is |
| `actionabilityScore` | 0-100 | Can the coach actually use this? |
| `safetyScore` | 0-100 | Could this insight cause harm if applied? |
| `noveltyScore` | 0-100 | Is this unique or duplicate of existing data? |

### Quality Thresholds

Insights are automatically filtered based on these thresholds:

```
MIN_QUALITY_SCORE:      60  ‚Üí Below this = rejected
MIN_SPECIFICITY_SCORE:  50  ‚Üí Below this = rejected
MIN_SAFETY_SCORE:       80  ‚Üí Below this = rejected
HUMAN_REVIEW_THRESHOLD: 75  ‚Üí Below this = needs human approval
```

### Scoring Flow

```
YouTube Interview Video
       ‚Üì
AI Extracts Insight
       ‚Üì
Score on 5 Dimensions
       ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ safetyScore < 80?        ‚Üí REJECT    ‚îÇ
   ‚îÇ qualityScore < 60?       ‚Üí REJECT    ‚îÇ
   ‚îÇ specificityScore < 50?   ‚Üí REJECT    ‚îÇ
   ‚îÇ qualityScore < 75?       ‚Üí HUMAN REVIEW ‚îÇ
   ‚îÇ All thresholds pass?     ‚Üí AUTO-APPROVE ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Example Scores

**High-scoring insight:**
```json
{
  "insight": "Users with ADHD respond better to body-based grounding than mental focus exercises because physical sensation anchors attention more effectively",
  "qualityScore": 85,
  "specificityScore": 90,
  "actionabilityScore": 88,
  "safetyScore": 95,
  "noveltyScore": 72
}
```

**Low-scoring insight (would be rejected):**
```json
{
  "insight": "People like feeling understood",
  "qualityScore": 35,
  "specificityScore": 20,
  "actionabilityScore": 30,
  "safetyScore": 100,
  "noveltyScore": 10
}
```

The second insight is too generic and obvious‚Äîit doesn't give the coach actionable guidance.

---

## Synthetic Data Generator

Since we can't get real user data until the prototype is ready, we can generate **synthetic training data** using Claude itself. This creates diverse, realistic coaching conversations without actual users.

### What Is Synthetic Data?

Synthetic data = AI-generated training examples that simulate real human interactions.

Instead of waiting for real users, we have Claude roleplay as diverse users with different:
- Cognitive styles (visual, verbal, systems thinkers)
- Neurological differences (ADHD, aphantasia, HSP)
- Emotional states (anxious, depressed, burnt out, stuck)
- Communication preferences (direct, exploratory, validating)
- Life situations (career crisis, relationship issues, grief)

### Why This Works

1. **Diversity control**: We explicitly generate edge cases real users might not show early on
2. **No privacy concerns**: It's synthetic, so no real user data needed
3. **Volume**: Can generate hundreds of examples quickly
4. **Quality control**: We define what "good" coaching looks like

### Generation Process

```
Define User Persona
      ‚Üì
Claude plays User + Coach
      ‚Üì
Generate 5-10 turn conversation
      ‚Üì
Score the coaching response quality
      ‚Üì
Keep high-quality examples
      ‚Üì
Use for fine-tuning
```

### Example Persona Prompt

```
You are simulating a user named Alex who has:
- ADHD (diagnosed 2 years ago)
- Works as a software engineer
- Currently overwhelmed by a job transition
- Communication style: Direct, gets impatient with small talk
- Needs: Actionable steps, variety in suggestions, validation that ADHD isn't a character flaw

Generate a realistic coaching conversation where Alex seeks help managing overwhelm during their job transition.
```

### Key Principles for Synthetic Generation

1. **Ground in real patterns**: Use insights from actual interviews to inform personas
2. **Include edge cases**: Generate scenarios the coach might struggle with
3. **Vary difficulty**: Some easy wins, some complex multi-session arcs
4. **Include failures**: Generate examples where coaching goes wrong to train on
5. **Diversity in demographics**: Age, culture, socioeconomic background, neurology

### Current Implementation

The Interview Processor can generate synthetic data by:

1. Taking existing approved insights
2. Creating realistic personas that would have those insights
3. Generating coaching conversations that demonstrate how to apply insights
4. Scoring and filtering the results

This creates a feedback loop: real insights ‚Üí synthetic examples ‚Üí better coaching ‚Üí better insights.

---

## Communication Style Analysis

The Interview Processor doesn't just extract **what** people say - it captures **how** they communicate. This is critical for training a coach that can adapt to different personalities and communication styles.

### Why This Matters

Generic AI responses feel robotic because they ignore communication style. A coach that understands:
- "This person speaks in rapid, terse sentences" ‚Üí Match their pace
- "This person is exploratory and verbose" ‚Üí Give them space to process
- "This person uses dry humor" ‚Üí It's safe to be lightly witty

### What We Extract

**1. Communication Style**

| Dimension | Options | What It Means |
|-----------|---------|---------------|
| `cadence` | rapid, measured, slow, variable | Speech rhythm/pacing |
| `verbosity` | terse, concise, moderate, verbose | How much they say |
| `directness` | very_direct, direct, exploratory, indirect | How they approach topics |
| `formality` | casual, conversational, professional, formal | Register |
| `emotionalExpression` | reserved, moderate, expressive, highly_expressive | How they show feeling |

**2. Personality Markers**

| Trait | Options | What It Reveals |
|-------|---------|-----------------|
| `thinkingStyle` | analytical, intuitive, practical, creative | How they process information |
| `socialEnergy` | introverted, ambivert, extroverted | Energy in interaction |
| `decisionMaking` | deliberate, balanced, spontaneous | How they choose |
| `conflictStyle` | avoidant, accommodating, direct, collaborative | How they handle tension |
| `humorStyle` | dry, self_deprecating, playful, observational, dark, none | Type of humor |

**3. Speech Patterns**

| Pattern | Example | Why It Matters |
|---------|---------|----------------|
| `fillerWords` | "um", "like", "you know" | Natural conversation markers |
| `catchPhrases` | "The thing is...", "Here's the deal" | Personal linguistic fingerprints |
| `sentenceStructure` | simple, compound, complex, varied | How they build thoughts |
| `questioningStyle` | rhetorical, genuine, leading, rare | How they engage |
| `storytellingStyle` | linear, tangential, dramatic, minimal | How they share experiences |

### Example Extraction

```json
{
  "insight": "When someone repeatedly trails off mid-sentence, they may be processing in real-time rather than having pre-formed thoughts",

  "communicationStyle": {
    "cadence": "variable",
    "verbosity": "moderate",
    "directness": "exploratory",
    "formality": "casual",
    "emotionalExpression": "expressive"
  },

  "personalityMarkers": {
    "thinkingStyle": "intuitive",
    "socialEnergy": "ambivert",
    "decisionMaking": "deliberate",
    "conflictStyle": "accommodating",
    "humorStyle": "self_deprecating"
  },

  "speechPatterns": {
    "fillerWords": ["like", "I mean", "sort of"],
    "catchPhrases": ["The thing is...", "I don't know if this makes sense but"],
    "sentenceStructure": "complex",
    "questioningStyle": "genuine",
    "storytellingStyle": "tangential"
  },

  "coachingImplication": "Give this person space to think out loud. Don't rush to fill silences. Reflect back what you hear to help them land on their point."
}
```

### Using This Data

The coach can use communication style data to:

1. **Match pacing** - Respond tersely to terse people, elaborate with verbose people
2. **Adjust formality** - Use casual language with casual communicators
3. **Honor processing style** - Let exploratory thinkers meander
4. **Mirror humor** - Use dry wit with dry humor people, warmth with earnest people
5. **Recognize personality** - Analytical people want reasons, intuitive people want resonance

---

## Audio Analysis for Aliveness

**Critical Limitation**: Text transcripts alone cannot capture aliveness qualities like cadence, rhythm, and pauses. The interview processor has audio analysis capabilities for extracting these features.

### What Audio Analysis Captures

| Feature | What It Measures | Why It Matters |
|---------|------------------|----------------|
| `speechRate` | Words per minute | Tempo baseline |
| `speechRateVariability` | How much rate changes | Imperfect rhythm marker |
| `pauseFrequency` | Pauses per minute | Natural latency |
| `pauseDuration` | How long pauses last | Thinking vs breathing |
| `volumeVariability` | Dynamic range | Amplitude restraint |
| `pitchContour` | Rising/falling/varied | Emotional expression |

### Aliveness Scores from Audio

The system calculates four aliveness scores from audio:

```
Imperfect Rhythm (0-100)
‚îî‚îÄ‚îÄ Derived from speech rate variability
    High variation = more alive, metronomic = robotic

Natural Latency (0-100)
‚îî‚îÄ‚îÄ Presence of thinking pauses (>2 seconds)
    Pauses indicate real processing, not script reading

Amplitude Restraint (0-100)
‚îî‚îÄ‚îÄ Lower average volume with dynamics
    Constant high volume = dramatic, restrained = human

Flow Quality (0-100)
‚îî‚îÄ‚îÄ Natural rhythm pattern
    Flowing > Variable > Steady > Staccato
```

### Current Status

**Audio analysis requires backend processing** (not yet implemented):

- Option 1: Backend service with Python/librosa
- Option 2: Integration with AssemblyAI or similar
- Option 3: Native module for on-device processing

Until implemented, aliveness qualities are **inferred from transcript** (less accurate):
- Filler words suggest natural rhythm
- Sentence fragments suggest variable pacing
- "..." or pauses in transcript hint at latency

### Workaround: Manual Observation

When reviewing interviews, manually note:
- Does the speaker pause to think?
- Is their pace variable or constant?
- Do they speak quietly or dramatically?
- Is there natural flow or choppy delivery?

Add these observations as custom aliveness markers in the insight.

---

### Limitations

- **Not a replacement**: Synthetic data supplements, doesn't replace real user data
- **Model bias**: Claude's synthetic users will have Claude's biases
- **Validation needed**: Real user testing still required before production
- **Quality variance**: Some generated conversations will be unrealistic

### Future Vision

```
Phase 1: Manual insight import (current)
Phase 2: YouTube harvesting + synthetic generation
Phase 3: Real user data collection (with consent)
Phase 4: Hybrid training (real + synthetic)
Phase 5: Self-improving loop
```

---

## Frequently Asked Questions

### "Do I need to install Llama to start training?"

**No.** The current training workflow does NOT require Llama to be installed.

Here's how the system works:

1. **Interview Processor** uses **Claude API** (cloud-based) to extract insights from YouTube videos
2. **Training Admin** stores insights locally on your device
3. **Version Control** manages training data versions

**Llama is only needed for the FUTURE phase** when you want to run a local AI model on-device. That phase is not yet implemented.

**Current workflow:**
```
YouTube Videos ‚Üí Claude API extracts insights ‚Üí Stored locally ‚Üí Used in prompts
```

**Future workflow (not implemented yet):**
```
Stored insights ‚Üí Export to JSON ‚Üí Fine-tune Llama with LoRA ‚Üí Run local model
```

### "Where is the Version Control UI?"

Go to: **Settings ‚Üí Developer Tools ‚Üí Version Control**

If you don't see it, make sure you've pulled the latest code and rebuilt the app:
```bash
git pull origin <your-branch>
npx expo start --clear
```

### "What is the Seeds tab?"

The **Seeds tab** is where users see pattern insights discovered from their data. Insights are presented as "seeds" that grow over time:

| Stage | Icon | Meaning |
|-------|------|---------|
| Sprouting | üå∞ | Just noticed, needs more data |
| Growing | üå± | Pattern becoming clearer |
| Flourishing | üåø | Strong, consistent pattern |
| Rooted | üå≥ | Core understanding |

**Where**: Main tab bar (between Skills and Insights)

### "How does the app discover insights?"

The app analyzes multiple data sources to find patterns:

1. **Twigs/Quick Logs** - Mood entries and notes
2. **Coach Conversations** - Topics and themes discussed
3. **Calendar Events** - Schedule patterns (with permission)
4. **Contacts** - Social interaction patterns (with permission)
5. **Location** - Movement patterns (with permission)
6. **Screen Time** - Digital habits (with permission)
7. **Health Data** - Sleep, steps, heart rate (with permission)
8. **Weather** - Environmental correlations

All analysis runs locally by default (no API needed).

### "What's the difference between Seeds and Insights tabs?"

| Seeds Tab | Insights Tab |
|-----------|--------------|
| Nature-based UI with growth metaphor | Charts and data visualizations |
| Pattern discoveries about YOU | Statistics and trends |
| Reactions (üå± ü§î üçÇ) to mark relevance | Graphs of mood over time |
| Coach can reference these naturally | Reference data for self-analysis |

Both help users understand themselves - Seeds focuses on discoveries, Insights focuses on data.

### "How do I add training insights vs. user insights?"

**Training Insights** (for AI learning):
- Added via Training Admin
- Shape how the AI responds to ALL users
- Example: "Users with anxiety prefer body-based techniques"

**User Insights** (Seeds):
- Discovered automatically from user data
- Personal patterns for ONE user
- Example: "You sleep better after evening walks"

They're separate systems with different purposes.

---

## Troubleshooting

### "Buttons don't work"
- Make sure all required fields are filled (Title, Insight, Coaching Implication)
- Check that JSON is valid (no trailing commas, proper quotes)

### "Import failed"
- Validate your JSON at jsonlint.com
- Check that `category` matches one of the valid categories exactly
- Check that `confidenceLevel` is one of: observed, inferred, validated, research_backed

### "Can't see Training Admin"
- Go to Settings ‚Üí Developer Tools ‚Üí Training Admin
- If missing, pull latest code and restart app

---

## Quick Copy Templates

### Single Insight (minimal)
```json
{
  "source": "Interview Notes",
  "insights": [
    {
      "category": "emotional_needs",
      "title": "YOUR TITLE HERE",
      "insight": "WHAT YOU LEARNED",
      "coachingImplication": "HOW AI SHOULD BEHAVE",
      "confidenceLevel": "observed"
    }
  ]
}
```

### Multiple Insights (batch)
```json
{
  "source": "Research Round 1",
  "insights": [
    {
      "category": "cognitive_patterns",
      "title": "Insight 1 title",
      "insight": "What you learned",
      "coachingImplication": "How AI should behave",
      "confidenceLevel": "observed"
    },
    {
      "category": "emotional_needs",
      "title": "Insight 2 title",
      "insight": "What you learned",
      "coachingImplication": "How AI should behave",
      "confidenceLevel": "observed"
    }
  ]
}
```

---

## Summary

1. **Single Import**: Fill form, click button - for one insight at a time
2. **Batch Import**: Paste JSON - for multiple insights at once
3. **Approve**: Review pending insights in Insights tab
4. **Export**: Get all data as JSON for training

The goal is to capture what you learn from real users so the AI can serve people better.
